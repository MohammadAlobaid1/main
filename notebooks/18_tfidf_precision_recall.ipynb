{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">\n",
    "    <i>\n",
    "        LIN 537: Computational Lingusitics 1 <br>\n",
    "        Fall 2019 <br>\n",
    "        Alëna Aksënova\n",
    "    </i>\n",
    "</div>\n",
    "\n",
    "# Notebook 18: tf-idf and precision/recall\n",
    "\n",
    "In this notebook, we discuss **tf-idf**, a way to assign a topic to a text.\n",
    "It stands for _term frequency -- inverse document frequency,_ and shows how important a word is for the meaning of the text. Then, in order to evaluate the obtained output, we will use **precision** and **recall**.\n",
    "\n",
    "Additionally, this is the last notebook, so in the end, I discuss the relevance of the topics covered in this class to different aspects and sub-fields of computational lingusitics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf-idf: term frequency -- inverse document frequency\n",
    "\n",
    "The **tf-idf** metric/alforithm contains two parts, **tf** and **idf**, that are being multiplied with each other.\n",
    "This score shows how important some word is for the meaning of a sentence or a text. \n",
    "\n",
    "\n",
    "Consider five following texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = \"\"\"Musk's warnings about artificial intelligence have brought\n",
    "him some controversy. He and Facebook founder Mark Zuckerberg have \n",
    "clashed, with the latter calling his warnings \"pretty irresponsible\". \n",
    "Musk responded to Zuckerberg's censure by saying that he had discussed \n",
    "AI with Zuckerberg and found him to have only a limited understanding of \n",
    "the subject. In 2014, Slate's Adam Elkus argued that current AIs were as \n",
    "intelligent as a toddler, and only in certain fields, going on to say \n",
    "that Musk's \"summoning the demon\" analogy may be harmful because it could \n",
    "result in significant cuts to AI research budgets.\"\"\"\n",
    "\n",
    "t2 = \"\"\"Like most members of the horse family, zebras are highly social. \n",
    "Their social structure, however, depends on the species. Mountain zebras \n",
    "and plains zebras live in groups, known as 'harems', consisting of one \n",
    "stallion with up to six mares and their foals. Bachelor males either live \n",
    "alone or with groups of other bachelors until they are old enough to \n",
    "challenge a breeding stallion. When attacked by packs of hyenas or wild dogs \n",
    "a zebra group will huddle together with the foals in the middle while the \n",
    "stallion tries to ward them off.\"\"\"\n",
    "\n",
    "t3 = \"\"\"The names of pseudo-ops often start with a dot to distinguish them \n",
    "from machine instructions. Pseudo-ops can make the assembly of the program \n",
    "dependent on parameters input by a programmer, so that one program can be \n",
    "assembled different ways, perhaps for different applications. Or, a pseudo-op \n",
    "can be used to manipulate presentation of a program to make it easier to read \n",
    "and maintain. Another common use of pseudo-ops is to reserve storage areas \n",
    "for run-time data and optionally initialize their contents to known values.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let us represent these texts as a dictionaries of words, where keys are the unique terms from the text, and values are the number of times that term is used in that text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from string import punctuation\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "data = []\n",
    "for t in [t1, t2, t3]:\n",
    "    \n",
    "    no_punct = []\n",
    "    for w in tokenizer.tokenize(t.lower()):\n",
    "        if w not in punctuation:\n",
    "            no_punct.append(w)\n",
    "    d = {w:no_punct.count(w) for w in no_punct}\n",
    "    data.append(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we obtained the list `data` describing a collection of $3$ documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    print(\"Text\", i+1)\n",
    "    print(data[i], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf (term frequency)\n",
    "\n",
    "First, we calculate how frequent some term is in the text that contains it.\n",
    "\n",
    "$$\\textrm{tf(word)} = \\frac{\\textrm{# of times this word appeared in this document}}{\\textrm{total # of terms in this document}}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(word, text):\n",
    "    assert word in text\n",
    "    words_total = sum(text.values())\n",
    "    return text[word] / words_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_t1 = {word:tf(word, data[0]) for word in data[0]}\n",
    "tf_t2 = {word:tf(word, data[1]) for word in data[1]}\n",
    "tf_t3 = {word:tf(word, data[2]) for word in data[2]}\n",
    "\n",
    "data = [tf_t1, tf_t2, tf_t3]\n",
    "\n",
    "print(tf_t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, variables `tf_t1`, `tf_t2` and `tf_t3` contain the information about frequencies of the words of the documents with respect to other words within the same document. However, the high values of some words within these dictionaries does not mean that these words are the topics of the text. **Why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### idf (inverse document frequency)\n",
    "\n",
    "Second, we need to determine how frequently a word is used in texts in general.\n",
    "\n",
    "$$\\textrm{idf(word)} = \\textrm{log}(\\frac{\\textrm{total # of documents}}{\\textrm{# of documents with this word in them}})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "def idf(word, documents):\n",
    "    mentions = sum([1 for i in documents if word in i])\n",
    "    return log(len(documents) / mentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now compute **idf** scores of all words across all documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(set([j for i in data for j in i]))\n",
    "idf_scores = {i:idf(i, data) for i in words}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf*idf\n",
    "\n",
    "Now, we can finally compute the tf-idf score for every term of every document.\n",
    "\n",
    "$$\\textrm{tf-idf(word)} = \\textrm{tf(word)} * \\textrm{idf(word)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in data:\n",
    "    for k in d:\n",
    "        d[k] *= idf_scores[k]\n",
    "        \n",
    "print(\"Text 1, for example:\")\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** how exactly were the $0$ scores derived?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to print, let's say, $3$ most important words for every one of those texts.\n",
    "First, to sort the obtained tf-idf values, we need to switch from a dictionary to another data type. **Why?**\n",
    "Then we can sort the container by first choosing word with a high tf-idf score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = []\n",
    "for i in data:\n",
    "    switched = [(k, i[k]) for k in i]\n",
    "    switched.sort(key = lambda x : x[1], reverse = True)\n",
    "    new_data.append(switched)\n",
    "    \n",
    "for i in range(len(new_data)):\n",
    "    print(\"Text\", i+1)\n",
    "    print(new_data[i][:3], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, some stop words still made it to the top of the tf-idf word lists.\n",
    "However, this is a problem only for very small datasets.\n",
    "\n",
    "After we obtained the scores for word in all the texts, this values serve as input to the next step. For example, we might want to calculate the summed and normalized **tf-idf for a sentence** checking how important that sentence is -- it shows if the sentence or its parts needs to be preserved when preparing the text summarization task. Alternatively, we can **label the text** as its top N words with respect to tf-idf -- for example, for automatic labeling of news articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluations: precision and recall\n",
    "\n",
    "Linguists in industry frequently face a task of evaluating a given model.\n",
    "\n",
    "\n",
    "**Formulate the task.** We have a model that that annotates tweets with one of the two labels: \"offensive\" and \"not offensive\". We also have $1000$ tweets that are annotated manually _(gold standard corpus)._\n",
    "\n",
    "**Get predictions of the model.** In order to evaluate predictions of the model, we need to get its predictions: automatically annotate the gold corpus.\n",
    "\n",
    "**Calculate precision and recall.** Those are the two metrics that evaluate the performance of models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/18_1.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **False negatives** are tweets that are claimed by the system to be not offensive, but they are, in fact, offensive;\n",
    "* **True positives** are correctly determined offensive tweets,\n",
    "* **True negatives** are correctly determined non-offensive tweets,\n",
    "* **False positives** are tweets that are claimed by the system to be offensive, but they are, in fact, not offensive.\n",
    "\n",
    "\n",
    "**Precision** evaluates how many or the selected items were, in fact, correct. It is also called _positive predictive value._\n",
    "\n",
    "$$ \\textrm{precision} = \\frac{\\textrm{# of tweets correctly annotated as offensive}}{\\textrm{total # of tweets annotated as offensive}} $$\n",
    "\n",
    "**Recall** checks how many relevant items were detected. It is also called _sensitivity._\n",
    "\n",
    "$$ \\textrm{recall} = \\frac{\\textrm{# of tweets correctly annotated as offensive}}{\\textrm{total # of offensive tweets}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practice.** For example, consider the following two annotations, where `golden` is referring to the golden corpus values, and `model` are the predictions of the model we are evaluating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "golden = [False, True, True, False, True, False, False,\n",
    "           True, False, False, True, False, False, True, \n",
    "           True, True, True, True, True, False]\n",
    "\n",
    "model = [False, False, True, False, False, True, False,\n",
    "           True, True, False, False, False, False, True, \n",
    "           True, True, True, True, False, False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the precision and recall of that model.\n",
    "\n",
    "_Part 1._ Calculate the precision of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Part 2._ Calculate the recall of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**F-score** is a measure of the annotation accurracy relying on precision and recall. It reaches its best value in $1$, and worst in $0$.\n",
    "\n",
    "$$\\textrm{F-score} = 2 * \\frac{\\textrm{precision} * \\textrm{recall}}{\\textrm{precision} + \\textrm{recall}}$$\n",
    "\n",
    "**Practice.** Calculate the F-score of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To sum up the semester..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
